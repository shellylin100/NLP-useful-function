# NLP useful function

# function to remove stopwords #
def remove_stopwords(texts):
  return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts] 
 
 
# Define a function lemmatization #
def lemmatization(texts, allowed_postags = ['NOUN', 'ADJ', 'VERB', 'ADV']):
  # Empty list for results
  texts_out = []
  # do the lemmatization and select pos tags:
  for sent in texts:
    # first convert into a list and then apply nlp() function
    doc = nlp(" ".join(sent))
    texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])
  return texts_out
 
 # Creating a tokenizer function: #
def spacy_tokenizer(sentence):
  # Creating our token object, which is used to create documents with linguistic annotations.
  mytokens = parser(sentence)
  # Lemmatizing each token and converting each token into lowercase
  mytokens = [ word.lemma_.lower().strip() if word.lemma_ != "PRON-" else word.lower_ for word in mytokens ]
  # Removing stop words
  mytokens = [word for word in mytokens if word not in stop_words and word not in punctuation]
  # return preprocessed list of tokens
  return mytokens 
