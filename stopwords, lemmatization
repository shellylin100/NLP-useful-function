# NLP useful function

# function to remove stopwords #
def remove_stopwords(texts):
  return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]â€©
 
 
# Define a function lemmatization #
def lemmatization(texts, allowed_postags = ['NOUN', 'ADJ', 'VERB', 'ADV']):
  # Empty list for results
  texts_out = []
  # do the lemmatization and select pos tags:
  for sent in texts:
    # first convert into a list and then apply nlp() function
    doc = nlp(" ".join(sent))
    texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])
  return texts_out
 
 # Creating a tokenizer function: #
def spacy_tokenizer(sentence):
  # Creating our token object, which is used to create documents with linguistic annotations.
  mytokens = parser(sentence)
  # Lemmatizing each token and converting each token into lowercase
  mytokens = [ word.lemma_.lower().strip() if word.lemma_ != "PRON-" else word.lower_ for word in mytokens ]
  # Removing stop words
  mytokens = [word for word in mytokens if word not in stop_words and word not in punctuation]
  # return preprocessed list of tokens
  return mytokens

# Custom transformer using spaCy
class predictors(TransformerMixin):
  def transform(self, X, **transform_params):
    # Cleaning Text
    return[clean_text(text) for text in X]
  
  def fit(self, X, y = None, **fit_params):
    return self
  
  def get_params(self, deep = True):
    return {}

# Basic function to clean the text
def clean_text(text):
  # Removing spaces and converting text into lowercase
  return text.strip().lower()
  
  
